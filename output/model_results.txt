
Logistic Regression Results:
Accuracy: 68.04%
Confusion Matrix:
[[434  44  86]
 [ 34 426 103]
 [150 126 296]]
Classification Report:
              precision    recall  f1-score   support

           0       0.70      0.77      0.73       564
           1       0.71      0.76      0.74       563
           2       0.61      0.52      0.56       572

    accuracy                           0.68      1699
   macro avg       0.68      0.68      0.68      1699
weighted avg       0.68      0.68      0.68      1699


Random Forest Results:
Accuracy: 89.82%
Confusion Matrix:
[[496   2  66]
 [  0 563   0]
 [101   4 467]]
Classification Report:
              precision    recall  f1-score   support

           0       0.83      0.88      0.85       564
           1       0.99      1.00      0.99       563
           2       0.88      0.82      0.85       572

    accuracy                           0.90      1699
   macro avg       0.90      0.90      0.90      1699
weighted avg       0.90      0.90      0.90      1699


XGBoost Results:
Accuracy: 88.70%
Confusion Matrix:
[[495   7  62]
 [  0 563   0]
 [111  12 449]]
Classification Report:
              precision    recall  f1-score   support

           0       0.82      0.88      0.85       564
           1       0.97      1.00      0.98       563
           2       0.88      0.78      0.83       572

    accuracy                           0.89      1699
   macro avg       0.89      0.89      0.89      1699
weighted avg       0.89      0.89      0.89      1699


LightGBM Results:
Accuracy: 90.46%
Confusion Matrix:
[[497   6  61]
 [  0 563   0]
 [ 89   6 477]]
Classification Report:
              precision    recall  f1-score   support

           0       0.85      0.88      0.86       564
           1       0.98      1.00      0.99       563
           2       0.89      0.83      0.86       572

    accuracy                           0.90      1699
   macro avg       0.90      0.91      0.90      1699
weighted avg       0.90      0.90      0.90      1699


Stacking (RF + LightGBM) Results:
Accuracy: 91.35%
Confusion Matrix:
[[496   1  67]
 [  0 563   0]
 [ 77   2 493]]
Classification Report:
              precision    recall  f1-score   support

           0       0.87      0.88      0.87       564
           1       0.99      1.00      1.00       563
           2       0.88      0.86      0.87       572

    accuracy                           0.91      1699
   macro avg       0.91      0.91      0.91      1699
weighted avg       0.91      0.91      0.91      1699

